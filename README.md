# 论文阅读知识库
优秀论文

学习进度：
 2025.8.1：通过对ai的总结和知识的提问，对于《GALLa Graph Aligned Large Language Models for Improved Source Code Understanding》的内容是，作者发现对于代码编程大模型中，可能模型并不理解代码里面的循环，选择，语法树等数据结构，模型能够正常解决我们的编程问题，就像是纯粹的靠一个程序员的直觉，的经验来实现的，其实它根本没有理解这个变量和其他变量的关系（这里或许解释的不恰当，但是意思就是不理解内部的含义关系把）；作者举例了语法树和数据流图的示例，这里作者就想到一个解决办法，那就是通过GNN的形式，去把这种关系表示在图网络中，作为一个数据和原始数据一起输入，这里是将图神经网络的输出通过交叉注意力投影到LLM的嵌入空间，和文本token拼接输入。后者就是作者提出的两阶段训练策略等等。
 
2025.8.11：《Trainable Dynamic Mask Sparse Attention.md》这篇论文还是比较有意思的，主要还是一种新的可训练动态掩码稀疏注意力，主要还是解决自注意力对于每个Token都去做一个注意力关系，这个是太耗费资源的，个人知识储备中的如informer论文中实验证明了其实每个Token之间并不是都是有关系的，只有部分有关系，而我们全都去做一个注意力的话，这样太耗费资源了，没必要。因此本文说的这种注意力对于在处理文本对话中是比较有作用的，因为对话随着轮数增加，token一直在增加，越到后面负担越重，使用这个稀疏注意力可以大大减少token。

2025.8.11:《Parameter-Efficient Multi-Task Model Fusion with Partial Linearization》这篇论文是一个关于大模型lora微调，然后使用线性化参数相加。具体意思就是在同等模型下微调各种任务，但是又不想各种任务彼此打扰（多任务学习中的干扰：在多个任务的学习中，任务特定的表示可能会相互干扰，导致任务之间的知识融合效果较差。简单的模型融合方法（例如加权平均）难以有效避免这种干扰。），因此使用了一种线性化的参数相加方法，对于每个lora微调的出来的任务参数在线性化相加就可以实现互不影响的功能。
